{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanlab Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from cleanlab.filter import find_label_issues\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx # Added for the network graph\n",
    "import logging\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration Constants (Using k=5 as requested)\n",
    "N_SPLITS = 5  # K for cross-validation\n",
    "BASE_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CLASSIFIER_MAX_ITER = 2000\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 16\n",
    "NUM_CONTRASTIVE_ITERATIONS = 20\n",
    "\n",
    "# Device Selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" # we're on a MB\n",
    "if not torch.cuda.is_available() and not torch.backends.mps.is_available():\n",
    "     DEVICE = \"cpu\"\n",
    "logger.info(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_train = pd.read_csv(\"dataset/train.tsv\",sep=\"\\t\")\n",
    "    df_eval = pd.read_csv(\"dataset/dev.tsv\",sep=\"\\t\")\n",
    "    df_test = pd.read_csv(\"dataset/test.tsv\",sep=\"\\t\")\n",
    "    df = pd.concat([df_train,df_eval,df_test])\n",
    "    del df_train, df_eval, df_test\n",
    "    logger.info(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Error: Dataset files not found. Make sure 'train.tsv', 'dev.tsv', and 'test.tsv' are in the 'dataset' directory.\")\n",
    "    exit() # Or handle error appropriately\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "logger.info(\"Preparing data...\")\n",
    "texts = df[\"text\"].tolist()\n",
    "noisy_labels_raw = df[\"label\"].values # Keep original potentially string labels if needed\n",
    "\n",
    "# Encode labels to integers if they are not already\n",
    "label_encoder = None # Initialize\n",
    "if not np.issubdtype(noisy_labels_raw.dtype, np.integer):\n",
    "    logger.info(\"Labels are not numeric. Applying LabelEncoder.\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    noisy_labels = label_encoder.fit_transform(noisy_labels_raw)\n",
    "    logger.info(f\"Label mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\\\")\")\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "else:\n",
    "    logger.info(\"Labels are already numeric.\")\n",
    "    noisy_labels = noisy_labels_raw.astype(int) # Ensure integer type\n",
    "    # label_encoder remains None\n",
    "    num_classes = len(np.unique(noisy_labels))\n",
    "\n",
    "logger.info(f\"Number of samples: {len(texts)}\")\n",
    "logger.info(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Loop to obtain OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold Cross-Validation Loop \n",
    "logger.info(f\"Starting Stratified {N_SPLITS}-Fold cross-validation to get OOF predictions...\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "# Initialize array to store out-of-fold predictions\n",
    "oof_preds = np.zeros((len(texts), num_classes), dtype=float)\n",
    "# Keep track of original indices corresponding to oof_preds rows\n",
    "original_indices = np.arange(len(texts))\n",
    "\n",
    "fold_counter = 0\n",
    "for train_index, val_index in skf.split(texts, noisy_labels):\n",
    "    fold_counter += 1\n",
    "    logger.info(f\"--- Starting Fold {fold_counter}/{N_SPLITS} ---\")\n",
    "\n",
    "    # Split Data for this Fold\n",
    "    train_texts = [texts[i] for i in train_index]\n",
    "    train_labels = noisy_labels[train_index]\n",
    "    val_texts = [texts[i] for i in val_index]\n",
    "    val_original_indices = original_indices[val_index]\n",
    "\n",
    "    # Convert training data to Hugging Face Dataset format for SetFitTrainer\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "    logger.info(f\"Fold {fold_counter}: Train size={len(train_texts)}, Validation size={len(val_texts)}\")\n",
    "\n",
    "    # Fine-tune Sentence Transformer Body using SetFitTrainer\n",
    "    logger.info(f\"Fold {fold_counter}: Fine-tuning SetFit body...\")\n",
    "    setfit_model_for_body_tuning = SetFitModel.from_pretrained(BASE_MODEL_NAME)\n",
    "    setfit_model_for_body_tuning.to(DEVICE)\n",
    "\n",
    "    trainer = SetFitTrainer(\n",
    "        model=setfit_model_for_body_tuning,\n",
    "        train_dataset=train_dataset,\n",
    "        num_iterations=NUM_CONTRASTIVE_ITERATIONS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=RANDOM_STATE,\n",
    "    )\n",
    "    trainer.train() # Runs contrastive body tuning + temporary head training\n",
    "\n",
    "    # Extract the Fine-tuned Body\n",
    "    fine_tuned_body = trainer.model.model_body.to(DEVICE)\n",
    "    logger.info(f\"Fold {fold_counter}: Body fine-tuning complete.\")\n",
    "\n",
    "    # Generate Embeddings using the Fine-tuned Body\n",
    "    logger.info(f\"Fold {fold_counter}: Generating embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        inference_batch_size = BATCH_SIZE * 2\n",
    "        train_embeddings = fine_tuned_body.encode(train_texts, convert_to_tensor=True, device=DEVICE, batch_size=inference_batch_size)\n",
    "        val_embeddings = fine_tuned_body.encode(val_texts, convert_to_tensor=True, device=DEVICE, batch_size=inference_batch_size)\n",
    "\n",
    "    train_embeddings_np = train_embeddings.cpu().numpy()\n",
    "    val_embeddings_np = val_embeddings.cpu().numpy()\n",
    "    logger.info(f\"Fold {fold_counter}: Embeddings generated.\")\n",
    "\n",
    "    # Train Logistic Regression Head Manually (Handling Skew)\n",
    "    logger.info(f\"Fold {fold_counter}: Training separate balanced Logistic Regression head...\")\n",
    "    manual_classifier_head = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=CLASSIFIER_MAX_ITER,\n",
    "        random_state=RANDOM_STATE,\n",
    "        solver='liblinear',\n",
    "        C=1.0\n",
    "    )\n",
    "    manual_classifier_head.fit(train_embeddings_np, train_labels)\n",
    "    logger.info(f\"Fold {fold_counter}: Manual head training complete.\")\n",
    "\n",
    "    # Predict Probabilities on Validation Set using Manual Head\n",
    "    logger.info(f\"Fold {fold_counter}: Predicting probabilities for validation set...\")\n",
    "    val_pred_probs = manual_classifier_head.predict_proba(val_embeddings_np)\n",
    "\n",
    "    # Store Out-of-Fold Predictions\n",
    "    oof_preds[val_original_indices] = val_pred_probs\n",
    "    logger.info(f\"Fold {fold_counter}: Stored OOF predictions.\")\n",
    "\n",
    "    # Cleanup Fold Resources\n",
    "    del setfit_model_for_body_tuning, trainer, fine_tuned_body, manual_classifier_head\n",
    "    del train_embeddings, val_embeddings, train_embeddings_np, val_embeddings_np, train_dataset\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    logger.info(f\"--- Finished Fold {fold_counter}/{N_SPLITS} ---\")\n",
    "\n",
    "logger.info(\"Cross-validation finished. Out-of-fold predictions generated for all samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cleanlab\n",
    "logger.info(\"Running cleanlab to find potential label issues...\")\n",
    "try:\n",
    "    label_issues_indices = find_label_issues(\n",
    "        labels=noisy_labels,\n",
    "        pred_probs=oof_preds,\n",
    "        return_indices_ranked_by='self_confidence', # Rank by confidence in given label\n",
    "    )\n",
    "    num_issues = len(label_issues_indices)\n",
    "    logger.info(f\"Cleanlab found {num_issues} potential label issues out of {len(texts)} samples.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error running cleanlab: {e}\")\n",
    "    logger.error(\"Cannot proceed without label issue indices.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect cleanlab results\n",
    "logger.info(\"\\n--- Analyzing Top Potential Label Issues ---\")\n",
    "N_ISSUES_TO_INSPECT = 50\n",
    "if num_issues > 0:\n",
    "    top_n_issues = min(N_ISSUES_TO_INSPECT, num_issues)\n",
    "    logger.info(f\"\\n--- Top {top_n_issues} Potential Label Issues (Ranked by 'self_confidence') ---\")\n",
    "\n",
    "    # Get the indices for the top N issues\n",
    "    top_issues_indices = label_issues_indices[:top_n_issues]\n",
    "\n",
    "    # Create DataFrame for inspection\n",
    "    issues_df = df.iloc[top_issues_indices].copy()\n",
    "    issues_df['cleanlab_rank'] = range(top_n_issues) # Add rank 0..N-1\n",
    "\n",
    "    # Get original labels (numeric and string if available)\n",
    "    original_numeric_labels_issues = noisy_labels[top_issues_indices]\n",
    "    if label_encoder:\n",
    "        issues_df['original_label'] = label_encoder.inverse_transform(original_numeric_labels_issues)\n",
    "    else:\n",
    "        issues_df['original_label'] = original_numeric_labels_issues\n",
    "\n",
    "    # Add predicted probability for the *given* label (self-confidence)\n",
    "    pred_prob_for_given_label = [oof_preds[idx, noisy_labels[idx]] for idx in top_issues_indices]\n",
    "    issues_df['pred_prob_for_given_label'] = pred_prob_for_given_label\n",
    "\n",
    "    # Add the label cleanlab would suggest (based on highest predicted probability)\n",
    "    suggested_numeric_label = np.argmax(oof_preds[top_issues_indices], axis=1)\n",
    "    if label_encoder:\n",
    "         issues_df['suggested_label'] = label_encoder.inverse_transform(suggested_numeric_label)\n",
    "    else:\n",
    "         issues_df['suggested_label'] = suggested_numeric_label\n",
    "\n",
    "    # Select and print columns for inspection\n",
    "    print(issues_df[['cleanlab_rank', 'text', 'original_label', 'suggested_label', 'pred_prob_for_given_label']].to_string())\n",
    "    logger.info(\"--- End Potential Issues List ---\")\n",
    "else:\n",
    "    logger.info(\"No potential label issues identified by cleanlab with current settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Conflict Matrix\n",
    "conflict_matrix = None\n",
    "class_names = []\n",
    "if num_issues > 0:\n",
    "    logger.info(\"\\n--- Calculating Conflict Matrix for Flagged Issues ---\")\n",
    "    # Get original and predicted labels *only for the flagged issues*\n",
    "    original_labels_for_issues = noisy_labels[label_issues_indices]\n",
    "    predicted_labels_for_issues = np.argmax(oof_preds[label_issues_indices], axis=1)\n",
    "\n",
    "    # Get class names consistently\n",
    "    if label_encoder:\n",
    "        class_names = label_encoder.classes_\n",
    "        if len(class_names) != num_classes:\n",
    "             logger.warning(f\"Mismatch between label_encoder classes ({len(class_names)}) and num_classes ({num_classes}). Using encoder classes.\")\n",
    "             num_classes = len(class_names) # Adjust num_classes if needed based on encoder\n",
    "    else:\n",
    "        class_names = [str(i) for i in range(num_classes)]\n",
    "\n",
    "    # Compute the confusion matrix if dimensions match\n",
    "    if len(class_names) == num_classes:\n",
    "        conflict_matrix = confusion_matrix(\n",
    "            y_true=original_labels_for_issues,\n",
    "            y_pred=predicted_labels_for_issues,\n",
    "            labels=np.arange(num_classes) # Ensure all classes 0..N-1 are included\n",
    "        )\n",
    "        logger.info(\"Conflict matrix calculated.\")\n",
    "    else:\n",
    "        logger.error(f\"Final class name length ({len(class_names)}) does not match num_classes ({num_classes}). Cannot create matrix.\")\n",
    "        conflict_matrix = None # Ensure it's None if calculation failed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conflict Heatmap\n",
    "logger.info(\"\\n--- Generating Basic Conflict Heatmap ---\")\n",
    "if conflict_matrix is not None:\n",
    "    # Create DataFrame for visualization (without sums)\n",
    "    conflict_df = pd.DataFrame(conflict_matrix, index=class_names, columns=class_names)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conflict_df, # Plot the raw matrix\n",
    "                annot=True,\n",
    "                fmt=\"d\",\n",
    "                cmap=\"viridis\",\n",
    "                linewidths=.5,\n",
    "                cbar=True)\n",
    "    plt.title('Label Conflict Heatmap (Counts of Original vs. Predicted Labels for Flagged Issues)')\n",
    "    plt.xlabel('Predicted Label (Suggested by Model)')\n",
    "    plt.ylabel('Original Label (From Noisy Dataset)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"label_conflict_heatmap_basic.png\") # Optional save\n",
    "    plt.show()\n",
    "    logger.info(\"Basic heatmap generated.\")\n",
    "else:\n",
    "    logger.info(\"Conflict matrix not available or empty, skipping heatmap generation.\")\n",
    "# --- END: Output 2 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Graph\n",
    "logger.info(\"\\n--- Generating Conflict Network Graph ---\")\n",
    "if conflict_matrix is not None and conflict_matrix.any():\n",
    "    G = nx.DiGraph() # Directed graph\n",
    "\n",
    "    # Add nodes (classes)\n",
    "    for i, name in enumerate(class_names):\n",
    "        # Optional: Calculate node attributes like total conflicts (can be used for size/color)\n",
    "        # total_outgoing = conflict_matrix[i, :].sum() - conflict_matrix[i, i]\n",
    "        # total_incoming = conflict_matrix[:, i].sum() - conflict_matrix[i, i]\n",
    "        # G.add_node(name, outgoing=total_outgoing, incoming=total_incoming)\n",
    "        G.add_node(name) # Simpler version without attributes for now\n",
    "\n",
    "    # Add edges (conflicts) - thicker/darker for more conflicts\n",
    "    max_conflict = conflict_matrix[~np.eye(num_classes, dtype=bool)].max() # Max off-diagonal value\n",
    "    min_conflict_display = 1 # Minimum count to draw an edge\n",
    "\n",
    "    edges_to_add = []\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            # Only add edges for off-diagonal conflicts above the minimum threshold\n",
    "            if i != j and conflict_matrix[i, j] >= min_conflict_display:\n",
    "                weight = conflict_matrix[i, j]\n",
    "                # Scale weight for visual thickness (adjust scaling factor '10' as needed)\n",
    "                scaled_viz_weight = 1 + 10 * (weight / max_conflict if max_conflict > 0 else 0)\n",
    "                edges_to_add.append((class_names[i], class_names[j], {'weight': weight, 'viz_weight': scaled_viz_weight}))\n",
    "\n",
    "    if not edges_to_add:\n",
    "        logger.info(\"No off-diagonal conflicts >= {min_conflict_display} found to draw in the network graph.\")\n",
    "    else:\n",
    "        G.add_edges_from(edges_to_add)\n",
    "        logger.info(f\"Added {len(edges_to_add)} edges to the network graph.\")\n",
    "\n",
    "        plt.figure(figsize=(14, 14)) # Adjust figure size as needed\n",
    "        # Use a layout algorithm (spring_layout is common)\n",
    "        pos = nx.spring_layout(G, k=0.9, iterations=50, seed=RANDOM_STATE) # Increase k for more spread\n",
    "\n",
    "        # Get edge widths from the 'viz_weight' attribute\n",
    "        edge_widths = [d['viz_weight'] for u, v, d in G.edges(data=True)]\n",
    "\n",
    "        # Draw the network components\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=700, node_color='lightblue', alpha=0.9)\n",
    "        nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='grey', alpha=0.6,\n",
    "                               arrows=True, arrowstyle='-|>', arrowsize=15, # Directed arrows\n",
    "                               connectionstyle='arc3,rad=0.1') # Slightly curved edges\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "        plt.title(\"Label Conflict Network Graph (Edge Thickness ~ Conflict Count for Flagged Issues)\")\n",
    "        plt.axis('off') # Hide the axes\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        logger.info(\"Network graph generated.\")\n",
    "\n",
    "elif conflict_matrix is not None and not conflict_matrix.any():\n",
    "     logger.info(\"Conflict matrix exists but is all zeros. No conflicts to draw.\")\n",
    "else:\n",
    "    logger.info(\"Conflict matrix not available, skipping network graph generation.\")\n",
    "\n",
    "logger.info(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
