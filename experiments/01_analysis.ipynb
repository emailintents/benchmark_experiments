{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "import itertools\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import umap.umap_ as umap\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in your environment in order load the pre-trained model\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and obtain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"banking\" # 'stackoverflow' 'banking' 'clinc'\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/padeck/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure stopwords are downloaded for lexical analysis\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"../data/{dataset_name}/train.tsv\",sep=\"\\t\")\n",
    "df_eval = pd.read_csv(f\"../data/{dataset_name}/dev.tsv\",sep=\"\\t\")\n",
    "df_test = pd.read_csv(f\"../data/{dataset_name}/test.tsv\",sep=\"\\t\")\n",
    "df = pd.concat([df_train, df_eval, df_test])\n",
    "del df_train\n",
    "del df_eval\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5487/13083 [01:15<01:37, 78.10it/s]"
     ]
    }
   ],
   "source": [
    "def calc_embeddings(text):\n",
    "    return model.encode(text,normalize_embeddings=True)\n",
    "\n",
    "df['text_embds'] = df['text'].progress_apply(calc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack embeddings into a 2D array\n",
    "embedding_matrix = np.vstack(df['text_embds'].values)\n",
    "\n",
    "# UMAP dimensionality reduction\n",
    "reducer = umap.UMAP(n_neighbors=30, min_dist=0.3, metric='cosine', random_state=42)\n",
    "embedding_2d = reducer.fit_transform(embedding_matrix)\n",
    "\n",
    "# Add 2D projection to original dataframe\n",
    "df['x'] = embedding_2d[:, 0]\n",
    "df['y'] = embedding_2d[:, 1]\n",
    "\n",
    "# Compute label-wise centroids in original embedding space\n",
    "label_centroids = df.groupby('label')['text_embds'].apply(\n",
    "    lambda x: np.mean(np.vstack(x.values), axis=0)\n",
    ")\n",
    "\n",
    "# Transform centroids to 2D using UMAP\n",
    "centroid_embeddings = np.vstack(label_centroids.values)\n",
    "centroid_2d = reducer.transform(centroid_embeddings)\n",
    "centroids_df = pd.DataFrame(centroid_2d, columns=['x', 'y'])\n",
    "centroids_df['label'] = label_centroids.index\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add data points grouped by label\n",
    "for label in df['label'].unique():\n",
    "    subset = df[df['label'] == label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=subset['x'],\n",
    "            y=subset['y'],\n",
    "            mode='markers',\n",
    "            name=str(label),\n",
    "            marker=dict(size=4, opacity=1.0),\n",
    "            text=subset['text'],\n",
    "            hoverinfo='text'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add centroids last — so they are on top\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centroids_df['x'],\n",
    "        y=centroids_df['y'],\n",
    "        mode='markers+text',\n",
    "        name='Centroids',\n",
    "        marker=dict(\n",
    "            color='black',\n",
    "            size=10,\n",
    "            symbol='x',\n",
    "            line=dict(width=2, color='white')\n",
    "        ),\n",
    "        text=centroids_df['label'],\n",
    "        textposition='top center',\n",
    "        textfont=dict(size=14, color='black'),\n",
    "        hoverinfo='text'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    title=f'UMAP Projection for the {dataset_name} dataset',\n",
    "    width=900,\n",
    "    height=700,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract nouns and verbs\n",
    "def extract_nouns_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.lemma_.lower() for token in doc if token.pos_ == 'NOUN' and not token.is_stop and token.is_alpha]\n",
    "    verbs = [token.lemma_.lower() for token in doc if token.pos_ == 'VERB' and not token.is_stop and token.is_alpha]\n",
    "    return nouns, verbs\n",
    "\n",
    "# Initialize result dictionary\n",
    "label_stats = {}\n",
    "\n",
    "# Enable tqdm for the outer loop over labels\n",
    "for label, group in tqdm(df.groupby('label'), desc=\"Processing Labels\"):\n",
    "    all_nouns = []\n",
    "    all_verbs = []\n",
    "\n",
    "    # tqdm for inner loop over texts in each label\n",
    "    for text in tqdm(group['text'], desc=f\"Texts in '{label}'\", leave=False):\n",
    "        nouns, verbs = extract_nouns_verbs(text)\n",
    "        all_nouns.extend(nouns)\n",
    "        all_verbs.extend(verbs)\n",
    "\n",
    "    noun_counts = Counter(all_nouns).most_common(3)\n",
    "    verb_counts = Counter(all_verbs).most_common(3)\n",
    "\n",
    "    label_stats[label] = {\n",
    "        'top_nouns': [word for word, _ in noun_counts],\n",
    "        'top_verbs': [word for word, _ in verb_counts]\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame\n",
    "result_df = pd.DataFrame.from_dict(label_stats, orient='index').reset_index()\n",
    "result_df.columns = ['label', 'top_nouns', 'top_verbs']\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Class Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings into a matrix\n",
    "embedding_matrix = np.vstack(df[\"text_embds\"].values)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "\n",
    "# Store in DataFrame for better readability\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=df.index, columns=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_label_stats = defaultdict(lambda: {\"Avg Similarity\": 0, \"Variance\": 0})\n",
    "\n",
    "labels = df['label'].unique()\n",
    "\n",
    "for label in labels:\n",
    "    indices = df[df[\"label\"] == label].index  # Get indices of this intent\n",
    "    sims = similarity_df.loc[indices, indices].values  # Extract similarity scores\n",
    "    np.fill_diagonal(sims, np.nan)  # Ignore self-similarity (diagonal values)\n",
    "    \n",
    "    avg_sim = np.nanmean(sims)  # Compute mean similarity\n",
    "    variance_sim = np.nanvar(sims)  # Compute variance\n",
    "    \n",
    "    intra_label_stats[label][\"Avg Similarity\"] = avg_sim\n",
    "    intra_label_stats[label][\"Variance\"] = variance_sim\n",
    "\n",
    "# Convert to DataFrame\n",
    "intra_df = pd.DataFrame.from_dict(intra_label_stats, orient=\"index\").reset_index()\n",
    "intra_df.columns = [\"Intent\", \"Avg Intra Similarity\", \"Variance\"]\n",
    "\n",
    "# Sort by average similarity\n",
    "intra_df = intra_df.sort_values(by=\"Avg Intra Similarity\", ascending=False)\n",
    "intra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging over all avg. intra class similarities for an overall metric\n",
    "intra_df['Avg Intra Similarity'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Category Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all unique intent labels\n",
    "intent_pairs = list(itertools.combinations(labels, 2))  # All possible intent pairs\n",
    "\n",
    "# Function to compute inter-intent similarity\n",
    "def compute_inter_intent_similarity(df, intent_a, intent_b):\n",
    "    # Extract embeddings for each intent\n",
    "    emb_a = np.stack(df[df[\"label\"] == intent_a][\"text_embds\"].values)\n",
    "    emb_b = np.stack(df[df[\"label\"] == intent_b][\"text_embds\"].values)\n",
    "    \n",
    "    # Compute cosine similarity between all utterances\n",
    "    return np.mean(cosine_similarity(emb_a, emb_b))\n",
    "\n",
    "# Compute similarity for all intent pairs and store in a dict\n",
    "inter_intent_similarities = {\n",
    "    (intent_a, intent_b): compute_inter_intent_similarity(df, intent_a, intent_b)\n",
    "    for intent_a, intent_b in intent_pairs\n",
    "}\n",
    "\n",
    "# Create a symmetric DataFrame for the similarity matrix\n",
    "intent_sim_matrix = pd.DataFrame(index=labels, columns=labels, dtype=float)\n",
    "\n",
    "# Populate the upper triangle of the matrix (no need to fill the lower triangle)\n",
    "for (intent_a, intent_b), sim in inter_intent_similarities.items():\n",
    "    intent_sim_matrix.loc[intent_a, intent_b] = sim\n",
    "    intent_sim_matrix.loc[intent_b, intent_a] = sim  # Symmetric\n",
    "\n",
    "# Optionally, fill diagonal values with NaN (since comparing same intents is not necessary)\n",
    "np.fill_diagonal(intent_sim_matrix.values, np.nan)\n",
    "\n",
    "# Display the resulting similarity matrix\n",
    "intent_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming centroid_sim_df is already defined as a similarity matrix\n",
    "# Mask the diagonal (self-similarity) by setting it to NaN\n",
    "np.fill_diagonal(intent_sim_matrix.values, np.nan)\n",
    "\n",
    "# Replace values that are 1 or 0 with NaN (self-similarity or exact dissimilarity)\n",
    "intent_sim_df = intent_sim_matrix.replace({1: np.nan, 0: np.nan})\n",
    "\n",
    "# Keep only the upper triangle to remove redundant pairs\n",
    "centroid_sim_df = intent_sim_df.where(np.triu(np.ones(intent_sim_df.shape), k=1).astype(bool))\n",
    "\n",
    "# Flatten the matrix, sort values, and get the top 100 highest similarity pairs\n",
    "most_similar = centroid_sim_df.stack().nlargest(100)\n",
    "\n",
    "\n",
    "# Display top 5 highest similarity pairs\n",
    "print(\"Most Similar Pairs:\")\n",
    "for (pair, similarity) in most_similar.items():\n",
    "    print(f\"The pair '{pair[0]}' and '{pair[1]}' has a cosine similarity of {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the most similar utterance-pair from the most similar cluster pair\n",
    "def find_most_similar_utterance_pair(df, top_clusters):\n",
    "    most_similar_utterances = []\n",
    "    \n",
    "    # Loop through the most similar cluster pairs\n",
    "    for cluster_1, cluster_2 in top_clusters:\n",
    "        # Extract the data for both clusters\n",
    "        cluster_1_entries = df[df['label'] == cluster_1]\n",
    "        cluster_2_entries = df[df['label'] == cluster_2]\n",
    "        \n",
    "        # Get the embeddings for the utterances in these clusters\n",
    "        cluster_1_embeddings = np.array(cluster_1_entries['text_embds'].tolist())\n",
    "        cluster_2_embeddings = np.array(cluster_2_entries['text_embds'].tolist())\n",
    "        \n",
    "        # Compute cosine similarities between the utterances from the two clusters\n",
    "        similarity_matrix = cosine_similarity(cluster_1_embeddings, cluster_2_embeddings)\n",
    "        \n",
    "        # Find the index of the most similar utterance-pair\n",
    "        idx_1, idx_2 = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)\n",
    "        \n",
    "        # Get the most similar utterance-pair and its similarity score\n",
    "        similarity_score = similarity_matrix[idx_1, idx_2]\n",
    "        \n",
    "        # Add to the results list with cluster labels\n",
    "        most_similar_utterances.append({\n",
    "            'cluster_1_label': cluster_1,  # Label of cluster 1\n",
    "            'cluster_2_label': cluster_2,  # Label of cluster 2\n",
    "            'cluster_1_utterance': cluster_1_entries.iloc[idx_1]['text'],  # Original text from cluster 1\n",
    "            'cluster_2_utterance': cluster_2_entries.iloc[idx_2]['text'],  # Original text from cluster 2\n",
    "            'similarity': similarity_score\n",
    "        })\n",
    "    \n",
    "    return most_similar_utterances\n",
    "\n",
    "# Get the top similar clusters (using the `top_5_similar` from previous code)\n",
    "# Assuming `top_5_similar.index` is a list of tuples: (cluster_1, cluster_2)\n",
    "top_clusters = most_similar.index\n",
    "\n",
    "# Find the most similar utterance-pair for the most similar clusters\n",
    "most_similar_utterances = find_most_similar_utterance_pair(df, top_clusters)\n",
    "\n",
    "# Display the most similar utterance-pair(s) for each cluster pair, including cluster labels\n",
    "print(\"\\nMost Similar Utterance Pair Between the Most Similar Clusters:\")\n",
    "for entry in most_similar_utterances:\n",
    "    print(f\"Cluster 1 (Label: '{entry['cluster_1_label']}') Utterance: '{entry['cluster_1_utterance']}'\")\n",
    "    print(f\"Cluster 2 (Label: '{entry['cluster_2_label']}') Utterance: '{entry['cluster_2_utterance']}'\")\n",
    "    print(f\"Cosine Similarity: {entry['similarity']:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBI and Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine distance\n",
    "def cosine_distance(v1, v2):\n",
    "    return 1 - np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Function to compute centroids of clusters\n",
    "def compute_centroids(df, label_col, emb_col):\n",
    "    centroids = []\n",
    "    labels = df[label_col].unique()\n",
    "    for label in labels:\n",
    "        cluster = np.vstack(df[df[label_col] == label][emb_col])\n",
    "        centroid = np.mean(cluster, axis=0)\n",
    "        centroid = normalize(centroid.reshape(1, -1))[0]\n",
    "        centroids.append(centroid)\n",
    "    return np.array(centroids), labels\n",
    "\n",
    "# Function to compute Davies-Bouldin Index using cosine distance\n",
    "def dbi_cosine(df, label_col, emb_col):\n",
    "    centroids, labels = compute_centroids(df, label_col, emb_col)\n",
    "    n_clusters = len(centroids)\n",
    "    S = np.zeros(n_clusters)\n",
    "    M = np.full((n_clusters, n_clusters), np.inf)\n",
    "\n",
    "    # Compute S_i and M_ij\n",
    "    for i in range(n_clusters):\n",
    "        cluster_i = np.vstack(df[df[label_col] == labels[i]][emb_col])\n",
    "        for x in cluster_i:\n",
    "            S[i] += cosine_distance(x, centroids[i])\n",
    "        S[i] /= len(cluster_i)\n",
    "\n",
    "        for j in range(i + 1, n_clusters):\n",
    "            M[i, j] = M[j, i] = cosine_distance(centroids[i], centroids[j])\n",
    "\n",
    "    # Compute DBI\n",
    "    DBI = np.mean([max((S[i] + S[j]) / M[i, j] for j in range(n_clusters) if i != j) for i in range(n_clusters)])\n",
    "    return DBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate DBI\n",
    "dbi_value = dbi_cosine(df, 'label', 'text_embds')\n",
    "print(f\"Davies-Bouldin Index (cosine): {dbi_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score already supports cosine as a distance metric\n",
    "silhouette_score(df['text_embds'].tolist(),df['label'].tolist(),metric='cosine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
